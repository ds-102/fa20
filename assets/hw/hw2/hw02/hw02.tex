\documentclass[a4paper,twoside,11pt]{article}
\usepackage{a4wide,graphicx,fancyhdr,clrscode,tabularx,amsmath,amssymb,color,enumitem,bbm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[algo2e]{algorithm2e}

%----------------------- Macros and Definitions --------------------------

\setlength\headheight{20pt}
\addtolength\topmargin{-10pt}
\addtolength\footskip{20pt}
\addtolength\parskip{0pt}

\fancypagestyle{plain}{%
\fancyhf{}
\fancyhead[LO,RE]{\sffamily Data 102 Assignment 2}
\fancyhead[RO,LE]{\sffamily Due: 11:59pm Monday, Oct 5, 2020}
\fancyfoot[LO,RE]{\sffamily } % Division of Data Science
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO,RE]{\sffamily Data 102 Assignment 2}
\fancyhead[RO,LE]{\sffamily Due: 11:59pm Monday, Oct 5, 2020}
\fancyfoot[LO,RE]{\sffamily } 
\fancyfoot[RO,LE]{\sffamily\bfseries\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0pt}

\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{}
\author{}
\date{}

\maketitle
\vspace{-8em}

\subsection*{Overview}

Submit your writeup including any code as a PDF via gradescope.\footnote{In Jupyter, you can download as PDF or print to save as PDF} We recommend reading through the entire homework beforehand and carefully using functions for testing procedures, plotting, and running experiments. Taking the time to reuse code will help in the long run!

Data science is a collaborative activity. While you may talk with others about the homework, please write up your solutions individually. If you discuss the homework with your peers, include their names on your submission. Please make sure any handwritten answers are legible, as we may deduct points otherwise.

\subsection*{1. Ridge as MAP}

    In this problem, we work through the maximum \textit{a posteriori} (MAP) interpretation of ridge regression. Suppose $x_1,\dots,x_n \in \mathbb{R}^d$ are fixed feature vectors. Assume the linear model, where we observe
\begin{align*} y_i =\beta^\top x_i + \varepsilon_i, \, i = 1, \ldots, n,
\end{align*}
where $\varepsilon_i \sim N(0,\sigma^2)$ are independent of each other, and $\beta\in\mathbb{R}^d$ and $\sigma^2 > 0$ are unknown. 

Let $y = (y_1,\dots,y_n), \varepsilon = (\varepsilon_1, \ldots, \varepsilon_n)$, and let $X$ denote the matrix whose $i$-th row is equal to $x_i$. Using this notation, we may more succinctly write the linear model as
$$
y = X\beta + \varepsilon, ~~\varepsilon\sim N(0, \sigma^2 I_n).
$$

We model the regression weights as a random variable with the following prior distribution:
\begin{align*}
    \beta \sim N (0, \sigma_\beta^2I_d).
\end{align*}
where $\sigma_\beta^2 > 0$ is hyperparameter we choose. 

\paragraph{(a)} Write the posterior distribution for $\beta$ after observing the data, $p(\beta | X,y)$. {\sl Hint:} use Bayes' rule and the probability density functions of multivariate Gaussians. Also use the fact that for a vector $z$, $z^Tz = \|z\|_2^2$, where $\|z\|_2$ is the Euclidean norm of $z$.
    
\paragraph{(b)} Show that the MAP estimator of $\beta$,
     \begin{align*}
         \hat{\beta}_{MAP} := \arg\max_\beta p(\beta | X, y)
     \end{align*}
     solves the regularized least-squares problem, 
     \begin{align*}
         \arg \min_\beta  \| X \beta - y \|_2^2 + \lambda \|\beta \|_2^2
     \end{align*}
     with $\lambda = \frac{\sigma^2}{ \sigma_\beta^2}$. {\sl Hint:} use part (a).
        
\paragraph{(c)} In the regularized least-squares problem, $\lambda$ is the regularization term: large values of $\lambda$ penalize weight vectors with large norms. Since $\hat\beta_{MAP}$ is the solution to the regularized least-squares problem with $\lambda = \frac{\sigma^2}{ \sigma_\beta^2}$, explain how our modeling decision (\textit{i.e.}, choice of $\sigma_\beta^2$) influences $\hat\beta_{MAP}$.  \newpage 
    
\subsection*{2. Rejection Sampling}

Consider the function 
\[
g(x) = \cos^2(12x)\times |x^3+6x-2| \times \mathbbm{1}_{x\in (-1, -.25)\cup (0, 1)}.
\]
In this problem, we use rejection sampling to generate random variables with pdf $f(x) = cg(x)$. 

\paragraph{(a)} Plot $g$ over its domain. What is a uniform proposal distribution $q$ that covers the support of $f$? What is a constant $M$ such that the scaled target distribution $p(x) = Mg(x)$ satisfies $p(x)\le q(x)$ for all $x$?

\paragraph{(b)} Suppose you run rejection sampling with target $p$ and proposal $q$ from part (a) until you generate $n$ samples and your sampler runs a total of $N \ge n$ times, including $n$ acceptances and $N-n$ rejections. Explain how you can use $n, N$ and $M$ to estimate $c$.

\paragraph{(c)} Use rejection sampling to generate a sample of size $10^3$ from $f$ and overlay a line plot of $f$ atop a normalized histogram of your samples. Repeat this step with $10^6$ samples. {\sl Hint:} to plot $f$, first use your values of $n, N$ and $M$ to estimate $c$ using your answer from part (b).

\subsection*{3. Gibbs Sampling} Graphical models are often useful for modeling phenomena involving multiple variables. In this problem, you'll formulate a graphical model, then demonstrate how to sample from the posterior using Gibbs sampling. 

\paragraph{(a)} Consider the following scenario: suppose the probability that a burglar breaks into your car is $\pi_b$, and the probability that an innocent passerby accidentally touches your car is $\pi_i$. Let $Z_b$ be a binary random variable that is $1$ if there is a burglar, and $0$ otherwise. Likewise, let $Z_i$ be a binary random variable that is $1$ if there is an innocent passerby, and $0$ otherwise. Suppose $Z_b$ and $Z_i$ are independent of each other.
        
        Let $X$ be a binary random variable that is $1$ if your car alarm goes off. The probability your car alarm goes off depends on $Z_b$ and $Z_i$, and is known to be:
\begin{table}[h!]
        \centering
         \begin{tabular}{|c|c|c|} 
         \hline
          $Z_b$ & $Z_i$ & $\mathbb{P}(X = 1 \mid Z_b, Z_i)$\\ 
         \hline
         0 & 0 & 0\\
         \hline
         0 & 1 & 0.05\\
         \hline
         1 & 0 & 0.85\\
         \hline
         1 & 1 & 0.90 \\
         \hline
         \end{tabular}
\end{table} 

\noindent Draw the graphical model depicting the direct relationships between $\pi_b$, $\pi_i$, $Z_b$, $Z_i$, and $X$.

\paragraph{(b)} Suppose you know the parameters $\pi_b$ and $\pi_i$, as well as $\mathbb{P}(X = 1 \mid Z_b, Z_i)$ as specified in Part (a). $X$ is the observed variable, and $Z_i$ and $Z_b$ are the latent (unobserved) variables. We want to sample from $\mathbb{P}(Z_i, Z_b \mid X, \pi_b, \pi_i)$, the posterior over the latent variables conditioned on everything else. We'll use Gibbs sampling to do this: 
\begin{enumerate}       
	\item[(i)] Suppose we are running Gibbs sampling, and on each iteration we sample $Z_b$ first and then sample $Z_i$. We observed $X = 0$, and the values of $Z_b$ and $Z_i$ from iteration $t$ are $Z_b^{(t)} = 0$ and $Z_i^{(t)} = 1$.
        
        Derive the distribution used for the Gibbs sampling update of $Z_b^{(t + 1)}$. Your solution should be in terms of $\pi_b$, $\pi_i$, and constants.

	\item[(ii)] Now, suppose we draw $Z_b^{(t + 1)} = 1$ from the distribution derived in Part (b.i). Derive the distribution used for the Gibbs sampling update of $Z_i^{(t + 1)}$. Your solution should be in terms of $\pi_b$, $\pi_i$, and constants.
\end{enumerate}

\iffalse
    \begin{parts}
        \part[2] Consider the following scenario: suppose the probability that a burglar breaks into your car is $\pi_b$, and the probability that an innocent passerby accidentally touches your car is $\pi_i$. Let $Z_b$ be a binary random variable that is $1$ if there is a burglar, and $0$ otherwise. Likewise, let $Z_i$ be a binary random variable that is $1$ if there is an innocent passerby, and $0$ otherwise. Suppose $Z_b$ and $Z_i$ are independent of each other.
        
        

        
        \newcommand{\bioneversion}[1]{
            \ifnumequal{#1}{1}{0.1}
            {\ifnumequal{#1}{2}{0.2}{0.05}}
        }
        \newcommand{\bitwoversion}[1]{
            \ifnumequal{#1}{1}{0.95}
            {\ifnumequal{#1}{2}{0.9}{0.85}}
        }
        \newcommand{\bithreeversion}[1]{
            \ifnumequal{#1}{1}{0.99}
            {\ifnumequal{#1}{2}{0.95}{0.9}}
        }
        \begin{table}[h!]
        \centering
         \begin{tabular}{|c|c|c|} 
         \hline
         $\PP(X = 1 \mid Z_b, Z_i)$ & $Z_b$ & $Z_i$ \\ 
         \hline
         0 & 0 & 0 \\
         \hline
         \bioneversion{\examversion} & 0 & 1 \\
         \hline
         \bitwoversion{\examversion} & 1 & 0 \\
         \hline
         \bithreeversion{\examversion} & 1 & 1 \\
         \hline
         \end{tabular}
        \end{table} 
        
        Draw the graphical model that describes the direct relationships between $\pi_b$, $\pi_i$, $Z_b$, $Z_i$, and $X$.
        \begin{solution}
        
        \begin{tikzpicture}[auto,node distance=1.5cm]
            \node[entity] (pib) {$\pi_b$};
            \node[coordinate](mid) [right = of pib] {};
            \node[entity] (zb) [below = of pib]{$Z_b$};
            \node[entity] (pii) [right = of mid]{$\pi_i$};
            \node[entity] (zi) [below = of pii]{$Z_i$};
            \node[coordinate](mid2)[below = of mid] {};
            \node[entity] (x) [below = of mid2]{$X$};
            \draw[->] (pib)--(zb);
            \draw[->] (pii)--(zi);
            \draw[->] (zb)--(x);
            \draw[->] (zi)--(x);
        \end{tikzpicture}
            
        \end{solution}
        \newpage 
        
        \part[5] Suppose you know the parameters $\pi_b$ and $\pi_i$, as well as $\PP(X = 1 \mid Z_b, Z_i)$ as specified in Part (a). $X$ is the observed variable, and $Z_i$ and $Z_b$ are the latent (unobserved) variables. We want to sample from $\PP(Z_i, Z_b \mid X, \pi_b, \pi_i)$, the posterior over the latent variables conditioned on everything else. We'll use Gibbs sampling to do this.
        
        (i) Suppose we are running Gibbs sampling, and on each iteration we sample $Z_b$ first then $Z_i$. We observed $X = 0$, and the values of $Z_b$ and $Z_i$ from iteration $t$ are $Z_b^{(t)} = 0$ and $Z_i^{(t)} = 1$.
        
        Derive the distribution used for the Gibbs sampling update of $Z_b^{(t + 1)}$. Your solution should be in terms of $\pi_b$, $\pi_i$, and constants.
        
        \begin{solution}
        For the Gibbs sampling update of $Z_b^{(t + 1)}$, we have
        \begin{align*}
            \PP(Z_b \mid Z_i, X, \pi_b, \pi_i) & = \frac{\PP(Z_b, Z_i, X \mid \pi_b, \pi_i)}{\PP(Z_i, X \mid \pi_b, \pi_i)} \\
            & = \frac{\PP(X \mid Z_b, Z_i, \pi_b, \pi_i) \PP(Z_b, Z_i \mid \pi_b, \pi_i)}{\PP(Z_i, X \mid \pi_b, \pi_i)} \\
            & = \frac{\PP(X \mid Z_b, Z_i) \PP(Z_b \mid \pi_b) \PP(Z_i \mid \pi_i)}{\PP(Z_i, X \mid \pi_b, \pi_i)} \\
            & = \frac{\PP(X \mid Z_b, Z_i) \PP(Z_b \mid \pi_b) \PP(Z_i \mid \pi_i)}{\sum_{z \in \{0, 1\}} \PP(Z_b=z, Z_i, X \mid \pi_b, \pi_i)} \\
            & = \frac{\PP(X \mid Z_b, Z_i) \PP(Z_b \mid \pi_b) \PP(Z_i \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X \mid Z_b=z, Z_i) \PP(Z_b=z \mid \pi_b)\PP(Z_i \mid \pi_i)}.
        \end{align*}
        Now we plug in the specific values given in the problem, including $X = 0$ and $Z_i = Z_i^{(t)} = 1$. Since $Z_b$ is a binary random variable, to find its distribution we can just find $\PP(Z_b = 1 \mid Z_i = 1, X = 0, \pi_b, \pi_i)$. Plugging in the values of $\PP(X \mid Z_b = 1, Z_i = 1)$ given in Part (a), and $\PP(Z_b = 1) = \pi_b$
        and $\PP(Z_i = 1) = \pi_i$, we have
        \ifnumequal{\examversion}{1} {
        \begin{align*}
            & \PP(Z_b = 1 \mid Z_i = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=z, Z_i = 1) \PP(Z_b=z \mid \pi_b)\PP(Z_i =1\mid \pi_i)} \\  
            & = \frac{0.01 \cdot \pi_b \pi_i}{0.9 \cdot (1 -\pi_b) \cdot \pi_i + 0.01 \cdot \pi_b \pi_i}.
        \end{align*}
        That is, $Z_b^{(t + 1)}$ is a Bernoulli random variable with probability of one equal to
        $$
        \frac{0.01 \cdot \pi_b \pi_i}{0.9 \cdot (1 -\pi_b) \cdot \pi_i + 0.01 \cdot \pi_b \pi_i}.
        $$
        }{
        \ifnumequal{\examversion}{2}{
        \begin{align*}
            & \PP(Z_b = 1 \mid Z_i = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=z, Z_i = 1) \PP(Z_b=z \mid \pi_b)\PP(Z_i =1\mid \pi_i)} \\  
            & = \frac{0.05 \cdot \pi_b \pi_i}{0.8 \cdot (1 -\pi_b) \cdot \pi_i + 0.05 \cdot \pi_b \pi_i}.
        \end{align*}
        That is, $Z_b^{(t + 1)}$ is a Bernoulli random variable with probability of one equal to
        $$
        \frac{0.05 \cdot \pi_b \pi_i}{0.8 \cdot (1 -\pi_b) \cdot \pi_i + 0.05 \cdot \pi_b \pi_i}.
        $$
        }
        {
        \begin{align*}
            & \PP(Z_b = 1 \mid Z_i = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=z, Z_i = 1) \PP(Z_b=z \mid \pi_b)\PP(Z_i =1\mid \pi_i)} \\  
            & = \frac{0.1 \cdot \pi_b \pi_i}{0.95 \cdot (1 -\pi_b) \cdot \pi_i + 0.1 \cdot \pi_b \pi_i}.
        \end{align*}
        That is, $Z_b^{(t + 1)}$ is a Bernoulli random variable with probability of one equal to
        $$
        \frac{0.1 \cdot \pi_b \pi_i}{0.95 \cdot (1 -\pi_b) \cdot \pi_i + 0.1 \cdot \pi_b \pi_i}.
        $$
        }
        }
        \end{solution}
        
        \newpage
        
        (ii) Now, suppose we draw $Z_b^{(t + 1)} = 1$ from the distribution derived in Part (b.i). Derive the distribution used for the Gibbs sampling update of $Z_i^{(t + 1)}$. Your solution should be in terms of $\pi_b$, $\pi_i$, and constants.
        
        \begin{solution}
        
        By the same reasoning as Part (b.i), for the Gibbs sampling update of $Z_i^{(t + 1)}$ we can focus on finding $\PP(Z_i = 1 \mid Z_b = 1, X = 0, \pi_b, \pi_i)$ (since we drew $Z_b^{(t + 1)} = 1$):
        \ifnumequal{\examversion}{1}{
        \begin{align*}
            & \PP(Z_i = 1 \mid Z_b = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=1, Z_i = z) \PP(Z_i=z \mid \pi_i)\PP(Z_b =1\mid \pi_b)} \\
            & = \frac{0.01 \cdot \pi_b \pi_i}{0.05 \cdot (1 - \pi_i)\pi_b + 0.01 \cdot \pi_b\pi_i}.
        \end{align*}
        }{
        \ifnumequal{\examversion}{2}{
        \begin{align*}
            & \PP(Z_i = 1 \mid Z_b = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=1, Z_i = z) \PP(Z_i=z \mid \pi_i)\PP(Z_b =1\mid \pi_b)} \\
            & = \frac{0.05 \cdot \pi_b \pi_i}{0.1 \cdot (1 - \pi_i)\pi_b + 0.05 \cdot \pi_b\pi_i}.
        \end{align*}
        }
        {
        \begin{align*}
            & \PP(Z_i = 1 \mid Z_b = 1, X = 0, \pi_b, \pi_i) \\
            & = \frac{\PP(X = 0\mid Z_b = 1, Z_i = 1) \PP(Z_b = 1 \mid \pi_b) \PP(Z_i = 1 \mid \pi_i)}{\sum_{z \in \{0, 1\}}\PP(X = 0\mid Z_b=1, Z_i = z) \PP(Z_i=z \mid \pi_i)\PP(Z_b =1\mid \pi_b)} \\
            & = \frac{0.1 \cdot \pi_b \pi_i}{0.15 \cdot (1 - \pi_i)\pi_b + 0.1 \cdot \pi_b\pi_i}.
        \end{align*}
        }
        }
        
        \end{solution}
\fi
%\subsection*{3. Modeling Prevalence}
  
  
\iffalse

\begin{thebibliography}{9}
\bibitem{efron2012large} 
Bradley Efron. 
\textit{Large-scale inference: empirical Bayes methods for estimation, testing, and prediction}. 
Cambridge University Press, 2012.
\end{thebibliography}

\fi

\end{document}