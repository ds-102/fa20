{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3:  Bayesian Estimation in Hierarchical Graphical Models\n",
    "Welcome to the third Data 102 lab! \n",
    "\n",
    "The code and responses you need to write are commented out with a message **\"TODO: fill in\"**. There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Gradescope Submission\n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday, Sep 22nd, 2020 at 11:59 PM. PST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import beta, binom\n",
    "import itertools\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "import hashlib\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beta-Binomial Graphical Model\n",
    "\n",
    "In this question we will look at the example that Prof. Steinhard talked briefly in class. \n",
    "\n",
    "In this problem we are trying to estimate the COVID infection risk in households. To do that we curate a list of K studies. Each study has an associated pair $(N_i, X_i)$ where $N_i$ denotes the number of susceptible individuals considered and $X_i$ is the number of them that became infected. In our modeling assumptions we assume that each susceptible person get's infected with probability $\\theta_i$. In epidemiology this quantity is known as Secondary Attack Rate or SAR for short.\n",
    "\n",
    "So let's say that we are trying to compare the studies and rank them in increasing order of infection risk. We would like to figure out the regions with the lowest SAR, as investigating them might direct us towards studying them more and understanding what contributed to their relative success. In the other direction we want to determine which are the regions with highest SAR because likely they most urgently are in need of intervention meassures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out a dataset \n",
    "study_df = pd.read_csv(\"study_df.csv\", header=0)\n",
    "print(study_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Compute the trivial estimate of SAR\n",
    "\n",
    "\n",
    "The most straingforward way to estimate the probability of infection (SAR) is to divide the number of infected cases to the number of susceptible cases. \n",
    "\n",
    "Compute this quantity in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the function\n",
    "def trivial_theta_estimate(N_value, X_value):\n",
    "    \"\"\"\n",
    "    Computes the trivial estimate of the Secondary Attack Rate\n",
    "    \n",
    "    Inputs:\n",
    "        N_value : int, number of susceptible individuals\n",
    "        X_value : int, number of infected individuals\n",
    "        \n",
    "    Output:\n",
    "        theta_est : float, estimate of probability of infection (SAR)\n",
    "    \"\"\"\n",
    "    theta_est = #TODO: fill in\n",
    "    return theta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests\n",
    "n_test_array = [10, 100, 1000]\n",
    "x_test_array = [10, 34, 852]\n",
    "res_array = [trivial_theta_estimate(n, x_test_array[i]) for i,n in enumerate(n_test_array)]\n",
    "hash_list=['e4c2e8edac362acab7123654b9e73432','149dd5056939405870c9bb50cbc8691c','83659da620f470d5a131b5a9c76cfee7']\n",
    "for i,res in enumerate(res_array):\n",
    "    assert get_hash(res) == hash_list[i]\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply  'trivial_theta_estimate' to each row of the dataframe and add a new column to hold the result\n",
    "study_df.apply(lambda row: trivial_theta_estimate(row['N'], row['X']), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trivial estimates suggest that both minimum and maximum probabilities of infection correspond to small studies.**\n",
    "\n",
    "|      | Max     | Min     |\n",
    "|------|---------|---------|\n",
    "| Name | Study 1 | Study 2 |\n",
    "| X    | 2       | 8       | \n",
    "| N    | 11      | 12      |\n",
    "|$\\theta$| 0.18  | 0.50    |\n",
    "\n",
    "\n",
    "Intuitively, we wouldn't want to make policy decision based on such small studies alone. We would like to balance between strong evidence of the small studies and high confidence in estimates from larger studies.\n",
    "\n",
    "Bayesian inference provides a flexible framework to balance our apriori beliefs with new evidence. Consider the following graphical model:\n",
    "\n",
    "\n",
    "![](GM1.png)\n",
    "\n",
    "\n",
    "Here are a few important quantities in Bayesian Inference. This lingo will be used at length in this course, so make sure you get familiar with it.  \n",
    "\n",
    "### Joint Density:\n",
    "The structure of the graphical model specified the full joint density of the parameters and data in the model. For this example the join density is:\n",
    "$$p(\\theta_1, \\theta_2, \\ldots, \\theta_K, X_1, \\ldots, X_K) = \\prod_{\\text{vertex $V$ in graph}}p(V|\\text{parent of $V$}) = \\prod_{i=1}^K \\underbrace{p(\\theta_i|\\alpha, \\beta)}_{\\text{prior of $\\theta_i$}} \\prod_{i=1}^K \\underbrace{p(X_i|\\theta_i)}_{\\text{likelihood of data $X_i$}}$$\n",
    "\n",
    "The factorization of the joint density into products of priors and likelihoods is the key featuere of Hierarchical Models. It allows to take a complex $2K$ dimensional joint probability and factorize it into products of $1$ dimensional probabilities.\n",
    "\n",
    "### Prior:  $\\theta_i \\sim Beta(\\alpha, \\beta)$\n",
    "We have the prior distribution: \n",
    "$$p(\\theta_i)= \\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)+\\Gamma(\\beta)}\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}$$ \n",
    "where $\\Gamma$ is the Gamma Function. Since the $\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)+\\Gamma(\\beta)}$ does not depend on the value of $\\theta$. It is a scaling factor that ensures that $p(\\theta_i)$ is a valid probability function. This leads to a common notation in practice: $p(\\theta_i)\\propto_{\\theta_i}\\theta_i^{\\alpha-1}(1-\\theta_i)^{\\beta-1}$. The symbol $\\propto_{\\theta_i}$ means _proportional in $\\theta_i$_.\n",
    "\n",
    "### Likelihood: $X_i|\\theta \\sim Binomial(N_i, \\theta_i)$\n",
    "Some textbooks denote the likelihood function as $p(X_i|\\theta)$, while other denote it as $p_{\\theta}(X_i)$. Here we will use the former.\n",
    "$$p(X_i|\\theta_i) = \\binom{N_i}{X_i} \\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}$$\n",
    "\n",
    "### Marginal: Unconditional distribution of $X_i$:\n",
    "$$p(X_i) = \\mathbb{E}_{\\theta}[p(X_i|\\theta_i)] = \\int_{0}^1 p(X_i|\\theta_i)p(\\theta_i)\\ d \\theta_i$$\n",
    "\n",
    "### Posterior: $\\theta_i|X_i$\n",
    "The goal of many estimation problems is to obtain the posterior distribution of the parameter of interest $\\theta_i$ with respect to the data $X_i$.\n",
    "\\begin{align}\n",
    "p(\\theta_i|X_i) &= \\frac{p(X_i|\\theta_i)p(\\theta_i)}{p(X_i)} \\quad \\text{by Bayes Rule}\\\\\n",
    "&\\propto_{\\theta} p(X_i|\\theta_i)p(\\theta_i) \\quad \\text{the marginal does not depend on $\\theta$}\\\\\n",
    "&\\propto_{\\theta}  \\underbrace{\\theta_i^{X_i}(1-\\theta_i)^{N_i - X_i}}_{\\text{likelihood}} \\underbrace{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}_{\\text{prior}}\\\\\n",
    "&\\propto_{\\theta}\\theta_i^{\\alpha + X_i - 1}(1-\\theta_i)^{\\beta + N_i - X_i - 1} \\quad \\text{unnormalized Beta density}\\\\\n",
    "\\end{align}\n",
    "\n",
    "### Hence $\\theta_i|X_i \\sim Beta(\\alpha + X_i, \\beta + N_i - X_i)$\n",
    "\n",
    "\n",
    "The fact that the posterior probability comes from the same distribution family is known as *conjugacy*. It is a very useful property because it allows us to compute the posteriors in close form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Examine the prior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(alpha_value, beta_value):\n",
    "    x = np.arange(0,1.01,0.01)\n",
    "    y = beta.pdf(x, alpha_value, beta_value)\n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Support')\n",
    "    plt.title('$\\\\theta_i \\sim Beta(\\\\alpha, \\\\beta)$')\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_beta, alpha_value=(1, 20, 0.5), beta_value=(1,20, 0.5))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b (i) Hold `alpha_value = 5`, slide `beta_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b (ii) Hold `beta_value = 5`, slide `alpha_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b (iii) Set `alpha_value = beta_value = 1`, increase their value such that `alpha_value=beta_value`. Write 1 sentence of your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Compute Posterior Mean Estimates for SAR\n",
    "In Problem 2 of Discussion 3 we showed that the **posterior mean** minimizes the **Bayes Risk** for tbe **Squared Error Loss**. _(if this sounds confusing take another look at the derivation of Problem 2a in Discussion 3)_. \n",
    "\n",
    "#### In the cell below write a function that computes the posterior mean corresponding to $\\theta_i|X_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def posterior_mean_estimate(N_value, X_value, alpha_value, beta_value):\n",
    "    \"\"\"\n",
    "    Computes the posterior E[theta_i|X_i] when we consider a prior theta_i ~ Beta(alpha, beta)\n",
    "    \n",
    "    Inputs: \n",
    "        N_value : int, total number of susceptible individuals\n",
    "        X_value : int, number of individuals that became infected\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "    \"\"\"\n",
    "    posterior_mean =  # TODO: fill in\n",
    "    return posterior_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests\n",
    "N_test = 100\n",
    "X_test = 20\n",
    "alpha_test_array = [1, 10, 100]\n",
    "beta_test_array = [1, 10, 100]\n",
    "inputs = list(itertools.product(alpha_test_array, beta_test_array))\n",
    "hash_list = ['8ae3cf8f9366cbdea2ccf7706546ba4a','f8ddc3234c0a54e55b01384bcfb23f90','82589ee1f18a2e0b9fe9d14836983102',\n",
    "             '08cf5a2033e7e21ec90b6707c24facaf','70da82175ec8a195a3d8b0fa8f69681d','ced20bed08ecfba035cbc3e06657cff2',\n",
    "             'c8a7feeaced214c662a999d9bf075e8c','790abc5c38e7c740420b03c24fabb05b','54fbf38cf649866815e0fefc46a1f6c7']\n",
    "for i,inp in enumerate(inputs):\n",
    "    assert hash_list[i] == get_hash(posterior_mean_estimate(N_test, X_test, *inp))\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Examine the posterior mean estimate\n",
    "\n",
    "Let's assume that from domain knowledge we expect probability of infection (SAR) to be around $\\frac{1}{3}$. We pick a prior distribution for $\\theta_i$'s that has mean $\\frac{1}{3}$. Any distributiond of the form $\\theta_i \\sim Beta(k, 2k)$ has this property. The value of $k$ determines the 'strenght' of the prior. Low values of $k$  correspond to 'flatter' priors, while larger values of $k$ correspond to 'peakier' priors. Play with the sliders in **1.b** to convince yourself.\n",
    "\n",
    "**Examine the plotting function below and answer the qualitative questions in the next cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify: Examine the code\n",
    "def plot_thetas(k):\n",
    "    \n",
    "    study_df[\"bayesian_theta\"] = study_df.apply(lambda row: posterior_mean_estimate(row['N'], row['X'], k, 2*k), axis =1)\n",
    "    study_df[\"trivial_theta\"] = study_df.apply(lambda row: trivial_theta_estimate(row['N'], row['X']), axis =1)\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    graph = sns.scatterplot(x=\"trivial_theta\", y=\"bayesian_theta\", data=study_df, size=\"N\", sizes=(50, 300), alpha=.8)\n",
    "    sns.lineplot(x='trivial_theta', y='trivial_theta', data= study_df,  ls=\"--\", color='k', lw= 1)\n",
    "    plt.ylim(0.16, 0.52)\n",
    "    graph.axhline(1/3,  ls=\"--\", color='k', label = \"$\\frac{1}{3}$ Prior Expectation\")\n",
    "    plt.xlabel('Trivial Estimate')\n",
    "    plt.ylabel('Posterior Mean Estimate')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    x = np.arange(0,1.01,0.01)\n",
    "    y = beta.pdf(x, k, 2*k)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Support')\n",
    "    plt.title('Prior $\\\\theta_i \\sim Beta(\\\\alpha={}, \\\\beta={})$'.format(k, 2*k))\n",
    "    plt.ylim(0, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_thetas, k=(0, 50, 2))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above the horizontal dashed line represents the prior mean estimate $\\mathbb{E}[\\theta_i] = \\frac{k}{k+2k} = 1/3$. The diagonal solid line marks $x=y$. Each data-point corresponds to a study, the size of the marker denotes the number of susceptible individuals in each study. Such that larger markers correspond to larger studies.\n",
    "\n",
    "**Answer the following questions with 1-2 sentences each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.d (i) Set $k=0$, what do you notice about the data points. Increase steadily the value of $k$. What happens with the points above the dashed line? What about the points below it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.d (ii) We see larger markers move 'slower' than smaller markers. How can you explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.d (iii) Imagine that we let $k\\to \\infty$. How do you think the two graphs above will look in the limit $k\\to \\infty$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    "### Note: DataHub is having issues with PyMC. We are trying to fix that. Download the notebook and run the rest on your local machine. We hope to address this issue soon.\n",
    "\n",
    "In the previous question we looked at a Beta-Binomial Graphical model. We took advantage of the conjugacy properties of the model and were able to compute close form solutions for the posterior mean estimates.\n",
    "\n",
    "However, as we introduce more complexity to the model, the conjugacy property quickly brakes and we have to resort to sampling techniques. Sampling techniques for approximate inference will be the topic of this week's lectures and next week's labs.\n",
    "\n",
    "Before delving deeper, in this question you will get a taste for probabilistic programming using `PyMC3`. Spend some time perusing the [documentation](https://docs.pymc.io/), but don't worry if don't really understand what's going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture install\n",
    "# ! pip install arviz==0.6.1\n",
    "# ! pip install pymc3==3.8\n",
    "# ! pip install Theano==1.0.4\n",
    "\n",
    "\n",
    "## If this fails uncomment the 4 lines above\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify: Spend some time examining the code\n",
    "def fit_approximate_inference(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\"\n",
    "    Creates and fits a PyMC3 model corresponding to the graphical model in Q.1\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "    \"\"\"\n",
    "    with pm.Model() as model:\n",
    "        theta = pm.Beta('theta', alpha=alpha_value, beta=beta_value, shape=len(study_df))\n",
    "        X = pm.Binomial('X', p=theta, observed=study_df['X'], n=study_df['N'])\n",
    "        trace = pm.sample(500, tune=1000, target_accept=0.95, random_seed = 0)\n",
    "    return(model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "def sample_posterior_theta(model, trace):\n",
    "    \"\"\"\n",
    "    Return samples from the posterior distribution theta_i|X_i\n",
    "    \n",
    "    Inputs:\n",
    "        models, trace : PyPM3 objects, outputs of fit_approximate_inference\n",
    "        \n",
    "    Output:\n",
    "        posterior_samples : (2000 x num_studies array), each column contains posterior samples for a theta_i\n",
    "    \"\"\"\n",
    "    with model:\n",
    "        posterior_samples = pm.sample_posterior_predictive(trace, var_names=[\"theta\"], random_seed=0)['theta']\n",
    "    return(posterior_samples)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Compute Empirical Posterior Mean Estimates from Samples\n",
    "\n",
    "Fill in the function that computes posterior mean estimates for $\\theta_i$'s for different parameters $\\alpha, \\beta$ of the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def empirical_posterior_mean_estimates(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\" \n",
    "    Computes posterior mean estimates of theta_i by performing approximate inference and then sampling from\n",
    "    the posterior distribution:\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Output:\n",
    "        posterior_estimates : (num_studies,) 1-D array of the same length as the number of studies,\n",
    "                               posterior_estimates[i] contains the empirical posterior mean estimate for theta_i\n",
    "    \n",
    "    \"\"\"\n",
    "    posterior_estimates = # TODO: fill in\n",
    "    return posterior_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests: Do not modify\n",
    "# Even if you don't pass the tests but get smth reasonably close to the numbers below you can consider it as passed\n",
    "\n",
    "# Study 0:  0.303 \n",
    "# Study 1:  0.261 \n",
    "# Study 2:  0.340 \n",
    "# Study 3:  0.307 \n",
    "# Study 4:  0.287 \n",
    "# Study 5:  0.329 \n",
    "# Study 6:  0.325 \n",
    "# Study 7:  0.282 \n",
    "# Study 8:  0.232 \n",
    "# Study 9:  0.372 \n",
    "# Study 10:  0.433 \n",
    "# Study 11:  0.352\n",
    "    \n",
    "posterior_estimates_test = empirical_posterior_mean_estimates(10,25)\n",
    "\n",
    "for i, est in enumerate(posterior_estimates_test):\n",
    "    print('Study {}:  {:.3f} '.format(i, est))\n",
    "\n",
    "hash_list = ['e85b79abfd76b7c13b1334d8d8c194a5','261943f3a93b683ceeac658927f3923f','149dd5056939405870c9bb50cbc8691c', \n",
    "             'ba6197788db60f5e2cb45cd403fa6559','246c0903b5a64b2a854ec1e7865f174f','ffa243f771800363714f6055d9236fd6', \n",
    "             '9f4721cf71c0ed18cd60356036b953cc','45efc23f34e05a9ea4f5024988047dd6','8f11bfb91ec29936603314c7cbc46119', \n",
    "             'a3f2a910685f5b07f5f45a5fc1fdb389','91afec64e32d6bf957e441df2ab638bb','8ce3fac7e23a02ab4e00cf0f1e03310a']\n",
    "for i, est in enumerate(posterior_estimates_test):\n",
    "    assert hash_list[i] == get_hash(est, 2)\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Plot the theoretical distribution of the posterior from Question 1 and the empirical distribution of the posterior from Question 2.\n",
    "\n",
    "Make 4x3 plot such that each subplot corresponds to a study. \n",
    "\n",
    "Each subplot should contain 2 curves and a frequency histogram:\n",
    "- The P.D.F of the prior distribution of $\\theta_i$\n",
    "- The P.D.F. of the posterior distribution $\\theta_i|X_i$ computed in closed form as in Q.1\n",
    "- The histogram of posterior samples of $\\theta_i|X_i$ computed in Q.2\n",
    "\n",
    "Make sure that you properly label each curve and histogram and give each subplot a meaningful title.\n",
    "\n",
    "To give you a mental image of what we have in mind here is a sample subplot:\n",
    "\n",
    "![](sample_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the plotting function\n",
    "def plot_densities(alpha_value, beta_value, study_df = study_df): \n",
    "\"\"\"\n",
    "    Plots for each study the prior distribution, theoretical posterior \n",
    "    and histogram of empirical posterior samples\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "        \n",
    "    Outputs:\n",
    "        fig : Figure with 12 subplots\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(4, 3)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    # TODO: fill in\n",
    "    \n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a weak prior\n",
    "fig1 = plot_densities(2, 4, study_df = study_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting densities for a strong prior\n",
    "fig2 = plot_densities(20, 40, study_df = study_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b (i) Compare the curve of the theoretical distribution with the histogram of samples from the empirical posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b (ii) Compare the two figures corresponding to 'weak' prior $\\theta_i \\sim Beta(2,4)$ and 'strong' prior  $\\theta_i \\sim Beta(20,40)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO:` fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Perform Approximate Inference on a more Complex Model\n",
    "\n",
    "The previous 2 parts served as a sanity check that the approximate inference techniques used by PyMC3 can approximate the theoretical posterior. The usefulness of such packages becomes apparent when we are dealing with more complex models that don't have conjugacy properties.\n",
    "\n",
    "Consider the following graphical model:\n",
    "\n",
    "![](GM2.png)\n",
    "\n",
    "Recent studies have shown that a large fraction of COVID cases do not show symptoms. The estimates of the asymptomatic rate fall in the range $[0.18, 0.43]$. We assume a prior $AR\\sim Uniform(0.18, 0.43)$\n",
    "\n",
    "All of the studies considered here tested only symptomatic cases, meaning that the probability that a person in a study tests positive is $\\theta_i*(1-AR)$. Hence:\n",
    "$$X_i|\\theta_i, AR \\sim Binomial(N_i, \\theta_i\\cdot AR)$$\n",
    "\n",
    "#### Complete `fit_approximate_inference_AR` function to add dependence on the asymptomatic rate:\n",
    "`Hint`: Consider [pm.Uniform](https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def fit_approximate_inference_AR(alpha_value, beta_value, study_df = study_df):\n",
    "    \"\"\"\n",
    "    Creates and fits a PyMC3 model corresponding to the graphical model above\n",
    "    \n",
    "    Inputs:\n",
    "        alpha_value, beta_value : floats, parameters of the prior Beta Distribution\n",
    "        study_df : DataFrame containing study data\n",
    "    \n",
    "    Outputs: (model, trace)\n",
    "    \"\"\"\n",
    "    with pm.Model() as model:\n",
    "        theta = # TODO: fill in\n",
    "        AR = # TODO: fill in\n",
    "        X = # TODO: fill in\n",
    "        trace = pm.sample(500, tune=1000, target_accept=0.95, random_seed=0)\n",
    "    return(model, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests: Do not modify\n",
    "# Even if you don't pass the tests but get smth reasonably close to the numbers below you can consider it as passed\n",
    "# Study 0:  0.374 \n",
    "# Study 1:  0.300 \n",
    "# Study 2:  0.444 \n",
    "# Study 3:  0.385 \n",
    "# Study 4:  0.359 \n",
    "# Study 5:  0.423 \n",
    "# Study 6:  0.415 \n",
    "# Study 7:  0.354 \n",
    "# Study 8:  0.285 \n",
    "# Study 9:  0.481 \n",
    "# Study 10:  0.556 \n",
    "# Study 11:  0.450\n",
    "\n",
    "model_test, trace_test = fit_approximate_inference_AR(5, 10)\n",
    "post_samples_test = sample_posterior_theta(model_test, trace_test)\n",
    "estimates = np.mean(post_samples_test, axis = 0)\n",
    "\n",
    "for i, est in enumerate(estimates):\n",
    "    print('Study {}:  {:.3f} '.format(i, est))\n",
    "\n",
    "\n",
    "hash_list = ['54fbf38cf649866815e0fefc46a1f6c7', 'e85b79abfd76b7c13b1334d8d8c194a5', \n",
    "             '54fbf38cf649866815e0fefc46a1f6c7', '54fbf38cf649866815e0fefc46a1f6c7', \n",
    "             '54fbf38cf649866815e0fefc46a1f6c7', '54fbf38cf649866815e0fefc46a1f6c7', \n",
    "             '54fbf38cf649866815e0fefc46a1f6c7', '54fbf38cf649866815e0fefc46a1f6c7', \n",
    "             'e85b79abfd76b7c13b1334d8d8c194a5', 'd310cb367d993fb6fb584b198a2fd72c', \n",
    "             'e95e1ca27d0e39aa03eb5a611ce4122f', 'd310cb367d993fb6fb584b198a2fd72c']\n",
    "\n",
    "\n",
    "for i, est in enumerate(estimates):\n",
    "    assert hash_list[i] == get_hash(est, 1)\n",
    "\n",
    "print('Test passed! You are awesome!')\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('baby_donkey.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
