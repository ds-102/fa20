{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Matching Markets {-}\n",
    "Welcome to the 9th DS102 lab! \n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "## Collaboration Policy {-}\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Gradescope Submission {-}\n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday, Nov 12th, 2020 at 11:59 PM. PST**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators {-}\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "sns.set(style=\"dark\")\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from gs_platform import Platform # <---- check out the gs_platform.py file in the lab folder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Markets {-}\n",
    "\n",
    "In this lab we will combine ideas from Multi Armed Bandits (MAB) and Stable Matching (SB). In Question 1 you will design a matching algorithm for the case when the preferences of both sides are known. In Question 2 we will extend the approach to consider cases when the preferences of the players are not known and have to be learned.\n",
    "\n",
    "This lab is inspired by the paper: [Liu, Lydia T., Horia Mania, and Michael Jordan. \"Competing bandits in matching markets.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.](https://arxiv.org/pdf/1906.05363.pdf)\n",
    "\n",
    "## Setup: {-}\n",
    "We will consider the following Matching Markets setup. There are $n$ players and $n$ arms. Let $\\mu_{ij}$ be the average reward arm $i$ gives to user $j$. This means that each arm $i$ has a preference over users. The most preferred player for arm $i$ is $\\arg\\max_j(\\mu_{ij})$ while the least preferred player $\\arg\\min_j(\\mu_{ij})$. Similarly each user $j$ has a preference over arms. We will assume that all average rewards $\\mu_{ij}$ are distinct, which means that all the preferences are *strict preferences*.\n",
    "\n",
    "In this setting, typically, the arms **know their own preferences** over players. Meaning that arm $i$ knows all the mean rewards $\\mu_{ij}, j =1\\ldots n$. However, the players **do not know their own preferences** over arms. This means that any algorithm designed to match players to arms must help players **learn their preferences**.\n",
    "\n",
    "First we will talk about two restrictions of this setup:\n",
    "\n",
    "### Multi-Armed Bandits (MAB): {-}\n",
    "If there is only one player and $n$ arms this problem becomes and instance of Multi-Armed Bandits. Take another look at Lab 8 to convince yourself that this is the case. In MAB problems the goal is to pick arms in a way that incurs as little Regret as possible when compared to the best arm in hindsight. Therefore, put another way, in a MAB problem the goal of the player is to learn their own preferences over arms as quickly as possible.\n",
    "\n",
    "### Stable-Matching (SM): {-}\n",
    "In multi-player settings, if each player knew their preferences, then, this becomes an instance of a Stable Matching problem. To see this connection with the SM example from lecture, imagine that the players are the *drummers* and the arms are the *bands*. The goal is to find a Matching between players and arms such that there are no blocking pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Stable Matching {-}\n",
    "\n",
    "In this question we will assume that each player $j$ knows the rewards associated with each of the arms: $\\{\\mu_{i,1}, \\mu_{i,2}, \\ldots, \\mu_{i,n}\\}$.\n",
    "\n",
    "#### Matching: {-}\n",
    "A Matching $M$ is a set of $n$ (player-arm) pairs of the form $(j', i')$. Such that:\n",
    "\n",
    "- Each player $j'$ appears in exacly one pair in $M$.\n",
    "- Each arm $i'$ appears in exactly one pair in $M$.\n",
    "\n",
    "#### Blocking Pair: {-}\n",
    "A pair $(j^*, i^*)\\notin M$, is a *blocking pair* for $M$ if both conditions hold:\n",
    "\n",
    "- Player $j^*$ would prefer arm $i^*$ over the current match in $M$.\n",
    "- Arm $i^*$ would prefer player $j^*$ over the current match in $M$.\n",
    "This means that a blocking pair would be better off breaking up with their matches and match with eachother.\n",
    "\n",
    "#### Stable Matching: {-}\n",
    "A Matching $M$ is *stable* if there are no blocking pairs.\n",
    "\n",
    "## Gale-Shapley Algorithm for Stable Matching {-}\n",
    "The Gale-Shapley Deferred Acceptance is an Algorithm to create stable matchings. The algorithm proceeds as follows:\n",
    "\n",
    "    INITIALIZE all ARMS and PLAYERS as FREE\n",
    "    INITIALIZE empty matching set M\n",
    "    WHILE (some arm i is unmatched):\n",
    "        LET j <- first player in the preference order to whom arm i has not made a proposal yet\n",
    "        IF (j is free);\n",
    "            ADD (j, i) to matching M\n",
    "        ELSE IF (j prefers i to current matching iâ€™)\n",
    "        Replace old (j, i') pair with new (j, i) in matching M\n",
    "    ELSE \n",
    "       j rejects i\n",
    "       \n",
    "### 1.a Identify stable matchings and blocking pairs {-}\n",
    "Run the code below to initialize the $\\mu_{i,j}$ parameters of a preference matrix. Identify which of the 3 proposed matches is stable. For the unstable mathches indentify the blocking pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to initialize preferences\n",
    "n = 4\n",
    "arm_names = ['Arm_{}'.format(i) for i in range(n)]\n",
    "player_names = ['Player_{}'.format(i) for i in range(n)]\n",
    "rewards_mat = np.array([6.5, 4.8, 5.7, 6.1, 8.2, 7.3, 6.2, 7.8, 5.7, 6.9, 5.5, 5.9, 7.6, 8.5, 5.7, 5.0]).reshape(n,n)\n",
    "rewards_df = pd.DataFrame(rewards_mat, columns=player_names)\n",
    "rewards_df.index = arm_names\n",
    "\n",
    "(rewards_df.style\n",
    "  .background_gradient(cmap=cm, axis = None)\n",
    "  .set_caption('Rewards of each arm-player pair')\n",
    "  .format(\"{:.2}\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TODO:` for each of the propossed matchings below, determine if the matching is stable. If the matching is unstable find at least one blocking pair. {-}\n",
    "\n",
    "- *(i)*: $M=\\{(Player_0, Arm_0), (Player_1, Arm_2), (Player_2, Arm_3), (Player_3, Arm_1)\\}$\n",
    "- *(ii)*: $M=\\{(Player_0, Arm_1),(Player_1, Arm_3),(Player_2, Arm_0),(Player_3, Arm_2)\\}$\n",
    "- *(iii)*: $M=\\{(Player_0, Arm_1),(Player_1, Arm_3),(Player_2, Arm_2),(Player_3, Arm_0)\\}$\n",
    "{0: 1, 1: 3, 2: 2, 3: 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO: fill in`\n",
    "\n",
    "- *(i)*:\n",
    "- *(ii)*:\n",
    "- *(iii)*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Implement Gale-Shapley Algorithm {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: Do not modify\n",
    "def get_preference_list(reward_list):\n",
    "    \"\"\"Given a list of rewards computes the preference list\n",
    "    \n",
    "    Inputs:\n",
    "        reward_list : list of floats containing rewards\n",
    "        \n",
    "    Outputs:\n",
    "        preference_list : list of int, which ranks the options in decreasing order of rewards\n",
    "            i.e. if preference_list = [2,0,1], it means that max(reward_list) = reward_list[2] and \n",
    "                                                             min(reward_list) = rewards_list[1]\n",
    "    \"\"\"\n",
    "    reward_list = np.array(reward_list)\n",
    "    preference_list = np.argsort(reward_list)\n",
    "    preference_list = preference_list[::-1]\n",
    "    return preference_list\n",
    "\n",
    "def is_new_preferred_to_old(preference_list, new, old):\n",
    "    \"\"\"Given a preference list determine whether the new option is preferred to the old option\n",
    "    \n",
    "    Inputs:\n",
    "        preference_list : list of int, of size n, where each element between 0 and n-1 occurs exactly once\n",
    "        new : int, index of the new option\n",
    "        old : int, index of the old option\n",
    "        \n",
    "    Outputs:\n",
    "        result : bool, returns True if new index is ahead of the old index in the preference list.\n",
    "        \"\"\"\n",
    "    \n",
    "    old_index_in_preference_list = list(preference_list).index(old)\n",
    "    new_index_in_preference_list = list(preference_list).index(new)\n",
    "    \n",
    "    return new_index_in_preference_list < old_index_in_preference_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Complete the function below to obtain the Arm-optimal matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: fill in\n",
    "# Hint: the helper function defined above could be useful in computing this function\n",
    "def get_gale_shapley_matching(arm_preference_dict, player_preference_dict):\n",
    "    \"\"\" Find the arm-proposing Gale-Shapley matching\n",
    "    \n",
    "    Inputs:\n",
    "        arm_preferance_dict : dict {int:list}\n",
    "            arm_preferance_dict[i] contains the preference list over players for arm i\n",
    "        player_preferance_dict : dict  {int:list}\n",
    "            player_preferance_dict[i] contains the preference list over arms for player i\n",
    "        how : string, default 'player'\n",
    "            one of 'player' or 'arm'\n",
    "    Outputs:\n",
    "        matching : dict, a dictionary of final matchings with n key-value pairs,\n",
    "            key is the player and the value is the arm in the matching\n",
    "            \n",
    "    \"\"\"\n",
    "    # get the number of arms\n",
    "    n = len(arm_preference_dict)\n",
    "    \n",
    "    # initialize the players and arms as unmatched\n",
    "    is_unmatched_player = np.array([True]*n)\n",
    "    is_unmatched_arm = np.array([True]*n)\n",
    "    \n",
    "    # initialize empty dictionary for current matchings\n",
    "    matching = {player:None for player in range(n)}\n",
    "    \n",
    "    # create a dictionary to record the rejected proposals\n",
    "    rejected_proposals ={arm:[] for arm in range(n)}\n",
    "    \n",
    "    # TODO: implement the logic in the while loop\n",
    "    while any(is_unmatched_arm) or any(is_unmatched_player):\n",
    "        curr_arm = np.where(is_unmatched_arm)[0][0] # index of the first arm in the current list of unmatched arms\n",
    "        \n",
    "        curr_player = # TODO: fill in\n",
    "                      # Hint: curr_player is the player to which the curr_arm is making a proposal\n",
    "                      #       it can be  computing by finding the most prefered player out of the ones that have\n",
    "                      #       not yet rejected this arm\n",
    "\n",
    "        if is_unmatched_player[curr_player]:\n",
    "            # create a matching\n",
    "            matching[curr_player] = curr_arm\n",
    "            # update the list of unmatched players and arms\n",
    "            is_unmatched_arm[curr_arm] = False\n",
    "            is_unmatched_player[curr_player] = # TODO: fill in\n",
    "        \n",
    "        else:\n",
    "            old_arm = matching[curr_player]\n",
    "            condition = # TODO: fill in\n",
    "                        # Hint: condition = True, when the proposing arm is a more preferred \n",
    "                        #       to the player than the old arm in the Matching\n",
    "            if condition:\n",
    "                # create a matching\n",
    "                matching[curr_player] = # TODO: fill in\n",
    "                # update the list of unmatched players and arms\n",
    "                is_unmatched_arm[curr_arm] = # TODO: fill in\n",
    "                is_unmatched_arm[old_arm] = # TODO: fill in\n",
    "            else:\n",
    "                rejected_proposals[curr_arm].append(curr_player)\n",
    "    return matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test\n",
    "# Compute the Arm Optimal Gale-Shaply matching for the example in 1.a.\n",
    "\n",
    "arm_preference_dict = {k: get_preference_list(rewards_mat[k,:]) for k in range(n)} \n",
    "player_preference_dict = {k: get_preference_list(rewards_mat[:,k]) for k in range(n)}\n",
    "\n",
    "matching = get_gale_shapley_matching(arm_preference_dict, player_preference_dict)\n",
    "print(matching)\n",
    "assert matching == {0: 1, 1: 3, 2: 2, 3: 0}\n",
    "print('Test Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More comprehensive validation tests\n",
    "test_arm_dict = {0: [1, 3, 0, 5, 2, 4],\n",
    "                 1: [3, 0, 1, 2, 5, 4],\n",
    "                 2: [3, 5, 1, 4, 2, 0],\n",
    "                 3: [0, 5, 4, 2, 3, 1],\n",
    "                 4: [4, 1, 5, 0, 2, 3],\n",
    "                 5: [5, 0, 2, 1, 3, 4]}\n",
    "test_player_dict = {0: [5, 3, 1, 0, 4, 2],\n",
    "                    1: [0, 2, 4, 1, 5, 3],\n",
    "                    2: [5, 1, 4, 3, 2, 0],\n",
    "                    3: [1, 0, 2, 3, 5, 4],\n",
    "                    4: [4, 2, 3, 1, 0, 5],\n",
    "                    5: [5, 2, 3, 0, 4, 1]}\n",
    "test_matching = get_gale_shapley_matching(test_arm_dict, test_player_dict)\n",
    "assert test_matching == {0: 3, 1: 0, 2: 2, 3: 1, 4: 4, 5: 5}\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. Competing Bandits {-}\n",
    "\n",
    "Now we assume that we are in the Competing Bandits setting, where players do not know their preference over arms. There goal of the players in to learn their preferences as quickly as possible. Here we will use the Centralized Gale-Shapley UCB proposed in Lydia's and Horia's [paper](https://arxiv.org/pdf/1906.05363.pdf).\n",
    "\n",
    "## Setup {-}\n",
    "\n",
    "At each time step the players are required to send a ranking of the arms to a centralized matching platform. The  platform takes in the playersâ€™ preferences at each time step and outputs a stable matching between the arms and players. The players pull their assigned arm and observe a reward. When player $j$ pulls arm $i$, the player gets a reward from $\\mathcal{N}(\\mu_{ij}, \\sigma_{ij})$. Therefore players must rank arms in a way which enables exploration and exploitation.\n",
    "\n",
    "## Centralized GS-UCB Algorithm {-}\n",
    "\n",
    "At each time-step $t$, keep track of the following:\n",
    "\n",
    "- $T_{ij}(t)$:  number of times player $j$ pulled arm $i$, up to time $t$.\n",
    "- $X_{ij}^{(1)},...,X_{ij}^{(T_{ij}(t))}$: the samples that player $j$ received from arm $i$ by time $t$. \n",
    "\n",
    "Let $\\hat \\mu_{ij}(t)$ be the mean of those samples: \n",
    "$$\\hat \\mu_{ij}(t) = \\frac{1}{T_{ij}(t)}\\sum_{k=1}^{T_{ij}(t)}X_{ij}^{(k)}$$\n",
    "\n",
    "Similarly to the previous lab, we define $C_{ij}(t, \\delta)$ the width of the confidence bound al level $\\delta$.\n",
    "$$C_{ij}(t, \\delta) = \\sigma_{ij}\\sqrt{\\frac{2\\log{1/\\delta}}{T_{ij}(t)}}.$$\n",
    "\n",
    "Similarly to Lab 8, if $T_{i,j} = 0$, then we set $\\hat \\mu_{ij}(t) = C_{ij}(t, \\delta) = \\infty$\n",
    "We will choose a $\\delta$ that decreases with time to ensure that the players explore the arms at first: \n",
    "$$\\delta=\\frac{1}{t^3}$$\n",
    "\n",
    "Each player $j$ computes a preference list over arms, by ranking the arms in decreasing order of upper confidence bounds $\\hat \\mu_{ij}(t) + C_{ij}(t, \\delta) \\ \\  i = 1\\ldots n$\n",
    "\n",
    "Subsequently, each player submits their preference lists to the centralized platform. The plaform runs the Gale Shapley matching algorithm to compute the matching.\n",
    "\n",
    "Finally each user gets to pull only 1 arm corresponding to the assigned match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Compute a preference list over arms given past history {-}\n",
    "`TODO`: fill in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete this function\n",
    "# Hint: Recall the UCB_pull_arm function you completed in the previous lab\n",
    "def get_UCB_preference_list(t, standard_deviations, times_pulled, rewards):\n",
    "    \"\"\" Implement the choice of arm for the UCB algorithm\n",
    "    \n",
    "    Inputs:\n",
    "        t : int, current iteration\n",
    "        standard_deviations : a list of length n (where n is the number of arms) of the \n",
    "            standard deviations associates with each arm\n",
    "        times_pulled: a list of length n (where n is the number of arms) of the number \n",
    "            of times each arm has been pulled.\n",
    "        rewards: a list of n lists. Each of the n lists holds the samples received from\n",
    "            pulling each arm up to iteration t. \n",
    "\n",
    "    Outputs:\n",
    "        preference_list: list of int, which ranks the arms in decreasing order of upper confidence bounds\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(times_pulled)\n",
    "    delta = 1/(t**3)\n",
    "    \n",
    "    confidence_bounds=[]\n",
    "    for arm in range(n):\n",
    "        if times_pulled[arm]==0:\n",
    "            confidence_bounds.append(np.inf)\n",
    "        else:\n",
    "            confidence_bounds.append(np.mean(rewards[arm])+standard_deviations[arm]*\n",
    "                                     np.sqrt((2/(times_pulled[arm]))*np.log(1/delta))) \n",
    "            \n",
    "    preference_list = # TODO: fill in\n",
    "                      # Hint: one of the helper functions defined earlier might be useful here\n",
    "        \n",
    "    return preference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests, do not modify\n",
    "times_pulled_test = [3, 5, 7, 4, 0]\n",
    "t_test = np.sum(times_pulled_test) + 1\n",
    "standard_deviations_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n",
    "rewards_test = [[10.4, 10.6, 11], \n",
    "                [8, 13, 12, 11, 9], \n",
    "                [9, 10, 10, 8, 9.5, 10.5, 11],\n",
    "                [8.3, 9.6, 7.9, 8.1],\n",
    "                []]\n",
    "preference_list = get_UCB_preference_list(t_test, standard_deviations_test, times_pulled_test, rewards_test)\n",
    "assert list(preference_list) == [4, 0, 1, 2, 3]\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret {-}\n",
    "\n",
    "At each time step the players produce preference lists and submit them to the centralized platform. The platform then computes stable matches according to the declared preferences of players and true preferences of arms via Gale-Shapley algorithm. Denote by $m_j(t)$ the match assigned to player $j$ at time $t$. Denote by $m_j^*$ the Arm-Optimal stable match for player $j$, if player $j$ knew their true preference over the arms.\n",
    "\n",
    "In this setting regret for each player is defined with respect to the 'true' stable matching $m^*$.\n",
    "\n",
    "Regret for player $j$ over time horizon $T$ is defined as:\n",
    "$$ Regret_j(T)= \\sum_{t=1}^T X_{m^*_j,j}^{(t)} - X_{m_j(t), j}^{(t)}$$\n",
    "\n",
    "Most of the time, it is simpler to analyze the __pseudo-regret__, which is the mean of the regret.\n",
    "\n",
    "$$ R_j(T)= T \\mu_{m^*_j, j} - \\mathbb{E}\\left[\\sum_{t=1}^T X_{m_j(t), j}^{(t)}\\right]$$\n",
    "\n",
    "In the cell below we simulate the Centralized GS-UCB Algorithms for the preferences defined below.\n",
    "We simulate a horizon of $T=5000$ steps. Assume that all $\\sigma_{ij} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time horizon \n",
    "T=5000\n",
    "\n",
    "# Initialize true rewards \n",
    "n = 4\n",
    "means_mat = np.array([6.5, 4.8, 5.7, 6.1, 8.2, 7.3, 6.2, 7.8, 5.7, 6.9, 5.5, 5.9, 7.6, 8.5, 5.7, 5.0]).reshape(n,n)\n",
    "sd = 1\n",
    "sd_mat = np.ones([n,n])*sd\n",
    "\n",
    "# Initilize Platform <- take a look at gs_platform.py file\n",
    "platform = Platform(means_mat, sd_mat, get_UCB_preference_list, get_gale_shapley_matching)\n",
    "\n",
    "# Run Competing Bandits Matching Market\n",
    "platform.run(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of times the players pulled each arm:\n",
    "times_pulled_df = pd.DataFrame.from_dict(platform.times_pulled, orient='columns', dtype=int)\n",
    "times_pulled_df.index = arm_names\n",
    "times_pulled_df.columns = player_names\n",
    "\n",
    "sm = {'Player_{}'.format(k): 'Arm_{}'.format(v) for k,v in platform.true_matching.items()}\n",
    "print(\"The true stable matching is: {}\".format(sm))\n",
    "(times_pulled_df.style\n",
    "  .background_gradient(cmap=cm, axis = None)\n",
    "  .set_caption('Number of times each arm-player pair is played')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify: Run to create plots\n",
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(6)\n",
    "\n",
    "# Plot the observed regret\n",
    "for player in range(n):\n",
    "    axs[0].plot(platform.regret[player], label='Player_{}'.format(player)) \n",
    "axs[0].set_xlabel('Time $t$')\n",
    "axs[0].set_ylabel('Observed Cumulative Regret')\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Observed cumulative regrets for each player in one simulation of GS-UCB\")\n",
    "\n",
    "# Plot percentage of stable matchings\n",
    "percentage_stable_at_time_t = [platform.stable_count[t]/t for t in range(1,T+1)]\n",
    "axs[1].plot(percentage_stable_at_time_t)\n",
    "plt.tight_layout()\n",
    "axs[1].set_xlabel('Time $t$')\n",
    "axs[1].set_ylabel('Percentage of stable matchings')\n",
    "axs[1].set_title('What percent of matchings up to time $t$ coincide with the true stable matching?')\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b Answer the following questions: {-}\n",
    "- (i) The cumulative regret of Player 0 and Player 1 is strictly increasing with time. Why is that the case?\n",
    "- (ii) The cumulative regret of Player 3 is strictly decreasing with time. Why is that the case?\n",
    "- (iii) The cumulative regret of Player 2 is sometimes increasing and sometimes decreasing. Why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO: fill in`\n",
    "- (i)\n",
    "- (ii)\n",
    "- (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('cute_flemish.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "print(\"Yay, you've made it to the end of Lab 9!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
